{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "departmental-concentration",
   "metadata": {
    "id": "contrary-capacity"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RikVoorhaar/optimization-II-2021/blob/master/notebooks/week10.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-skiing",
   "metadata": {
    "id": "educated-dodge"
   },
   "source": [
    "# Week 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-alpha",
   "metadata": {
    "id": "perceived-occasions"
   },
   "source": [
    "## Exercise 1\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Consider the following convex problem\n",
    "$$\n",
    "\t\\begin{array}{ll}\n",
    "\t\\mbox{minimize} & f_0(x)\\\\\n",
    "\t\\mbox{subject to} & f_i(x)\\leq0\\,,\\quad i=1,\\ldots,m\\,.\n",
    "\t\\end{array}\n",
    "$$\n",
    "    \n",
    "where all $f_i$ are differentiable. Assume that $x^*\\in\\mathbb{R}^n$ and $\\lambda^*\\in\\mathbb{R}^m$ satisfy the KKT conditions. Prove by using properties of convex functions and the KKT conditions that\n",
    "$$\n",
    "\t\\nabla f_0(x^*)^T(x-x^*)\\geq0\n",
    "$$\n",
    "\n",
    "for all feasible $x$. (We have seen a similar result when $x^*$ is the global minimum. Here you need to prove it for a point that satisfies the KKT conditions.)\n",
    "\n",
    "_Hint: Show that_ $(x^*,\\lambda^*)$ _satisfies_\n",
    "$$\\sum_{i=1}^m \\lambda_i^*(f_i(x^*)+\\nabla f_i(x^*)^\\top (x-x^*)) = -\\nabla f_0(x^*)^\\top (x-x^*)$$\n",
    "\n",
    "_for any_ $x$_, and use feasibility of_ $x$ _to prove that the left-hand side is negative._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-alignment",
   "metadata": {
    "id": "-IP_idMOKvdb"
   },
   "source": [
    "Since $x$ is feasible we have $f_i(x)\\leq 0$, then by convexity of $f_i$:\n",
    "$$\n",
    "f_i(x^*)+\\nabla f_i(x^*)^\\top (x-x^*) \\leq f_i(x)\\leq 0\n",
    "$$\n",
    "\n",
    "In addition to primal feasibility, the KKT conditions tell us that the dual is feasible: $\\lambda^*\\geq 0$, the Lagrangian gradient vanishes: $\\nabla L(x^*,\\lambda^*)=0$, and the slackness condition holds: $\\sum_i \\lambda^*_i f_i(x^*)=0$. Now we consider the sum\n",
    "$$\n",
    "0 \\geq \\sum_{i=1}^m \\lambda_i^*(f_i(x^*)+\\nabla f_i(x^*)^\\top (x-x^*))\n",
    "$$\n",
    "\n",
    "Using slackness, the first term vanishes. Now note that\n",
    "$$\n",
    "\\sum_i\\lambda_i\\nabla f_i(x^*)^\\top (x-x^*)) = [\\nabla L(x^*,\\lambda^*)-\\nabla f_0(x^*)]^\\top (x-x^*)\n",
    "$$\n",
    "\n",
    "Since the Lagrangian gradient vanishes, we therefore have \n",
    "$$\n",
    "0\\geq -\\nabla f_0(x^*)^\\top (x-x^*),\n",
    "$$\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-outline",
   "metadata": {
    "id": "still-trail"
   },
   "source": [
    "## Exercise 2: Inequalities for steepest descent\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "In this exercise we will derive some inequalities that will be important for the analysis of gradient descent.\n",
    "\n",
    "Recall that for $\\beta>0$ a function $f\\colon \\mathbb R^n\\to \\mathbb R$ is called $\\beta$-smooth if $f$ is differentiable and for all $x,y\\in \\mathbb R^n$ we have the following inequality:\n",
    "$$\n",
    "    \\| \\nabla f(x) - \\nabla f(y)\\|\\leq \\beta\\|x-y\\|\n",
    "$$\n",
    "    \n",
    "(in other words, the gradient is $\\beta$-Lipschitz).\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-voluntary",
   "metadata": {
    "id": "exposed-hampton"
   },
   "source": [
    "### Exercise 2a)\n",
    "\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $f$ be $\\beta$-smooth, show that for all $x,y\\in \\mathbb R^n$ we have \n",
    "$$\n",
    "    |f(x)-f(y)-\\nabla f(y)^\\top (x-y)|\\leq \\frac\\beta2\\|x-y\\|^2\n",
    "$$\n",
    "\n",
    "_Hint: Write_ $f(x)-f(y)$ _as_ \n",
    "$$\n",
    "\\int_0^1 \\frac{d}{dt} f(y+t(x-y))\\,\\mathrm dt= \\int_0^1\\nabla f(y+t(x-y))^\\top (x-y)\\,\\mathrm dt,\n",
    "$$\n",
    "    \n",
    "_Use this to write the entire expression under the absolute value signs as an integral over an inner product_ $\\int_0^1 a(t)^\\top b(t)\\,\\mathrm dt$_, and apply Cauchy-Schwarz to this inner product._\n",
    "\n",
    "_Remark: If we also assume that_ $f$ _is convex, then we can drop the absolute value sign, since this expression is always positive for convex functions._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-priest",
   "metadata": {
    "id": "JogNJngtVdK3"
   },
   "source": [
    "As in the hint, we write \n",
    "$$\n",
    "\\begin{align}\n",
    "|f(x)-f(y)-\\nabla f(y)^\\top (x-y)| &= \\left|\\int_0^1 \\nabla f(y+t(x-y))^\\top(x-y)-\\nabla f(y)^\\top (x-y)\\,\\mathrm dt\\right|\\\\\n",
    "&= \\left|\\int_0^1 \\left\\langle \\nabla f(y+t(x-y))-\\nabla f(y),\\,x-y\\right\\rangle\\,\\mathrm dt\\right|\\\\\n",
    "&\\leq \\int_0^1 \\|\\nabla f(y+t(x-y))-\\nabla f(y)\\|\\cdot\\|x-y\\|\\,\\mathrm dt\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now the left term can be bounded using $\\beta$-smoothnes by $\\|\\nabla f(y+t(x-y))-\\nabla f(y)\\|\\leq t\\beta \\|(x-y)\\|$, giving\n",
    "$$\n",
    "\\begin{align}\n",
    "|f(x)-f(y)-\\nabla f(y)^\\top (x-y)| &\\leq \\int t\\beta \\|x-y\\|^2\\,\\mathrm dt\\\\\n",
    "&= \\frac\\beta2\\|x-y\\|^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-sarah",
   "metadata": {
    "id": "infectious-indie"
   },
   "source": [
    "### Exercise 2b)\n",
    "\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Assume $f$ is convex and $\\beta$-smooth. Show that for all $x,y\\in\\mathbb R^n$ we have\n",
    "$$\n",
    "    f(x)-f(y)\\leq \\nabla f(x)^\\top (x-y)-\\frac1{2\\beta} \\|\\nabla f(y)-\\nabla f(x)\\|^2\n",
    "$$\n",
    "\n",
    "_Hint: define_ $z=y-\\frac1\\beta (\\nabla f(y)-\\nabla f(x))$ _and consider_ \n",
    "$$f(x)-f(y) = (f(x)-f(z))+(f(z)-f(y)).\n",
    "$$\n",
    "    \n",
    "_Use convexity to bound the first term, and use the inequality of 2a) to bound the second term._\n",
    "\n",
    "_Remark: We only need_ $\\beta$_-smoothness here to use the inequality of 2a), so if_ $f$ _is convex and satisfies the inequality of 2a), it also satisfies the inequality of this exercise. This observation will be useful in 2d)._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-sample",
   "metadata": {
    "id": "Qzzo8AocaZOR"
   },
   "source": [
    "As suggested by the hint, we have by convexity that\n",
    "$$\n",
    "f(x)-f(z)\\leq \\nabla f(x)^\\top (x-z)\n",
    "$$\n",
    "\n",
    "Then by 2a) we have that\n",
    "$$\n",
    "f(z)-f(y)\\leq \\nabla f(y)^\\top (z-y) +\\frac{\\beta}{2}\\|z-y\\|^2\n",
    "$$\n",
    "\n",
    "Since $z=y-\\frac1\\beta (\\nabla f(y)-\\nabla f(x))$ we have $\\frac\\beta2\\|z-y\\|^2=\\frac{1}{2\\beta}\\|\\nabla f(x)-\\nabla f(y)\\|^2$. Bringing this together, we obtain\n",
    "$$\n",
    "\\begin{align}\n",
    "    f(x)-f(y)&\\leq \\nabla f(x)^\\top (x-z) +\\nabla f(y)^\\top (z-y)+\\frac{1}{2\\beta}\\|\\nabla f(x)-\\nabla f(y)\\|^2\\\\\n",
    "    &= \\nabla f(x)^\\top (x-y) +(\\nabla f(x)-\\nabla f(y))^\\top (y-z)\\\\&\\qquad\\qquad+\\frac{1}{2\\beta}\\|\\nabla f(x)-\\nabla f(y)\\|^2\\\\\n",
    "    &= \\nabla f(x)^\\top (x-y) +(\\nabla f(x)-\\nabla f(y))^\\top \\frac1\\beta(\\nabla f(y)-\\nabla f(x))\\\\&\\qquad\\qquad+\\frac{1}{2\\beta}\\|\\nabla f(x)-\\nabla f(y)\\|^2\\\\\n",
    "    &= \\nabla f(x)^\\top (x-y)- \\frac{1}{2\\beta}\\|\\nabla f(x)-\\nabla f(y)\\|^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-equilibrium",
   "metadata": {
    "id": "civic-socket"
   },
   "source": [
    "### Exercise 2c)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Prove the converse of 2b), that is, prove that if $f:\\mathbb R^n\\to \\mathbb R$ is differentiable and if for all $x,y\\in\\mathbb R^n$ we have \n",
    "$$\n",
    "    f(x)-f(y)\\leq \\nabla f(x)^\\top (x-y)-\\frac1{2\\beta} \\|\\nabla f(y)-\\nabla f(x)\\|^2\n",
    "$$\n",
    "\n",
    "then $f$ is convex and $\\beta$-smooth. \n",
    "\n",
    "Hint: _To show_ $\\beta$_-smoothenss, use add the inequality of 2b) for_ $x,y\\in\\mathbb R^n$ **and** _for_ $y,x\\in \\mathbb R^n$, _to show the stronger result that_\n",
    "$$\n",
    "\\|\\nabla f(y)-\\nabla f(x)\\|^2\\leq  \\beta (\\nabla f(x)-\\nabla f(y))^\\top (x-y)\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-sending",
   "metadata": {
    "id": "mOljZ-WLkPq7"
   },
   "source": [
    "Convexity is obtained by ignoring the second term in the inequality above. \n",
    "\n",
    "For $\\beta$-smoothness we add the bound above for $(x,y)$ and $(y,x)$ to obtain \n",
    "$$\n",
    "0\\leq (\\nabla f(x)-\\nabla f(y))^\\top (x-y)-\\frac{1}{\\beta}\\|\\nabla f(y)-\\nabla f(x)\\|^2\n",
    "$$\n",
    "\n",
    "Then by Cauchy-Schwarz we obtain\n",
    "$$\n",
    "\\frac1\\beta\\|\\nabla f(y)-\\nabla f(x)\\|^2\\leq  (\\nabla f(x)-\\nabla f(y))^\\top (x-y)\\leq \\|\\nabla f(y)-\\nabla f(x)\\|\\|y-x\\|\n",
    "$$\n",
    "\n",
    "Giving $\\beta$-smoothness:\n",
    "$$\n",
    "\\|\\nabla f(y)-\\nabla f(x)\\|\\leq \\beta\\|y-x\\|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-arrest",
   "metadata": {
    "id": "twenty-comment"
   },
   "source": [
    "### Exercise 2d)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Suppose that $f\\colon \\mathbb R^n\\to \\mathbb R$ is $\\alpha$-strongly convex, and $\\beta$-smooth. Show that for all $x,y\\in\\mathbb R^n$ we have\n",
    "$$\n",
    "    (\\nabla f(x)-\\nabla f(y))^\\top (x-y) \\geq \\frac{\\alpha\\beta}{\\beta+\\alpha}\\|x-y\\|^2 +\\frac{1}{\\beta+\\alpha}\\|\\nabla f(x) - \\nabla f(y)\\|^2\n",
    "$$\n",
    "\n",
    "This result is useful for improving an easier convergence result of gradient descent. \n",
    "\n",
    "_Hint: Use_ $\\alpha$_-strong convexity to show that_ $\\phi(x):=f(x)-\\frac\\alpha2 \\|x\\|^2$ _is convex and_ $(\\beta-\\alpha)$_-smooth. To show_ $(\\beta-\\alpha)$_-smoothness, rewrite the inequality of 2a) in terms of_ $\\phi$. _Use this to show that_ $\\phi$ _satisfies the inequality of 2c), and is hence_ $(\\beta-\\alpha)$_-smooth. Then finally apply the stronger inequality mentioned in 2c) to_ $\\phi$ _to obtain the required result after some algebraic manipulation._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-sandwich",
   "metadata": {
    "id": "-8n_2MEOjmuI",
    "tags": []
   },
   "source": [
    "As per the hint, we consider $\\phi(x) = f(x)-\\frac\\alpha2\\|x\\|^2$. \n",
    "Recall that $f$ is $\\alpha$-strongly convex if\n",
    "$$\n",
    "f(x)-f(y) \\leq \\nabla f(x)^\\top (x-y) -\\frac\\alpha2\\|x-y\\|^2\n",
    "$$\n",
    "\n",
    "Therefore $\\phi$ is convex precisely if $f$ is $\\alpha$-strictly convex. Now to show $\\phi$ is $(\\beta-\\alpha)$-smooth, we use the fact that $f$ is $\\beta$ smooth, and the result of 2a):\n",
    "<!-- $$\n",
    "    f(x)-f(y)\\leq \\nabla f(x)^\\top (x-y)-\\frac1{2\\beta} \\|\\nabla f(y)-\\nabla f(x)\\|^2\n",
    "$$ -->\n",
    "$$\n",
    "    |f(x)-f(y)-\\nabla f(y)^\\top (x-y)|\\leq \\frac\\beta2\\|x-y\\|^2\n",
    "$$\n",
    "\n",
    "By convexity of $f$ we can drop the absolute value signs in this inequality. \n",
    "Now replacing $f(x)$ by $\\phi(x)+\\frac\\alpha2\\|x\\|^2$ and $\\nabla f(x)$ by $\\nabla \\phi(x)+\\alpha x$, we get\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac\\beta2\\|x-y\\|^2&\\geq \\phi(x)-\\phi(y)+\\frac\\alpha2(\\|x\\|^2-\\|y\\|^2)-\\nabla \\phi(y)^\\top (x-y)-\\alpha y^\\top (x-y)\\\\\n",
    "    &=\\phi(x)-\\phi(y)+\\frac\\alpha2\\left(\\|x\\|^2+\\|y\\|^2+2y^\\top x\\right)-\\nabla \\phi(y)^\\top(x-y)\\\\\n",
    "    &=\\phi(x)-\\phi(y)-\\nabla \\phi(y)^\\top(x-y)+\\frac\\alpha2\\|x-y\\|^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore \n",
    "$$\n",
    "\\phi(x)-\\phi(y)-\\nabla \\phi(y)^\\top (x-y)\\leq \\frac{\\beta-\\alpha}{2}\\|x-y\\|^2\n",
    "$$\n",
    "\n",
    "By the remark of 2b), This is all we need to prove that $\\phi$ satisfies the inequality of 2b), and hence by extension, that it is $(\\beta-\\alpha)$-smooth. Then by 2c) we have\n",
    "$$\n",
    "\\|\\nabla \\phi(y)-\\nabla \\phi(x)\\|^2\\leq  (\\beta-\\alpha) (\\nabla \\phi(x)-\\nabla \\phi(y))^\\top (x-y)\n",
    "$$\n",
    "\n",
    "Rewriting this back in terms of $f$ we get\n",
    "$$\n",
    "\\|\\nabla f(y)-\\nabla f(x)+\\alpha(x-y)\\|^2\\leq  (\\beta-\\alpha)(\\nabla f(x)-\\nabla f(y) +\\alpha(y-x))^\\top (x-y)\n",
    "$$\n",
    "\n",
    "Hence\n",
    "$$\n",
    "\\|\\nabla f(y)-\\nabla f(x)+\\alpha(x-y)\\|^2 +\\alpha(\\beta-\\alpha)\\|x-y\\|^2\\leq  (\\beta-\\alpha) (\\nabla f(x)-\\nabla f(y))^\\top (x-y)\n",
    "$$\n",
    "\n",
    "Now we expand the first term\n",
    "$$\n",
    "\\begin{align}\n",
    "\\|\\nabla f(y)-\\nabla f(x)\\|^2+(\\alpha^2+\\alpha(\\beta-\\alpha))&\\|x-y\\|^2-2\\alpha (\\nabla f(x)-\\nabla f(y))^\\top (x-y)\\\\\n",
    "&\\leq  (\\beta-\\alpha) (\\nabla f(x)-\\nabla f(y))^\\top (x-y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thenn finally, this gives\n",
    "$$\n",
    "(\\nabla f(x)-\\nabla f(y))^\\top (x-y) \\geq \\frac{1}{\\alpha+\\beta}\\|\\nabla f(y)-\\nabla f(x)\\|^2+\\frac{\\alpha\\beta}{\\alpha+\\beta}\\|x-y\\|^2\n",
    "$$\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-nirvana",
   "metadata": {
    "id": "incoming-monroe"
   },
   "source": [
    "## Exercise 3: Boolean least squares\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $A\\in \\mathbb R^{m\\times n}$ and $b\\in \\mathbb R^m$. We consider the Boolean least squares problem\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    \\text{minimize} & \\|Ax-b\\|^2\\\\\n",
    "    \\text{subject to} & x_i\\in \\{-1,\\,1\\}, \\quad i=1,\\dots,n\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This is not a convex problem, and we thus want to relax it to a convex problem giving a useful lower bound.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-trigger",
   "metadata": {
    "id": "atomic-mexico"
   },
   "source": [
    "### Exercise 3a)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Show that the Boolean least squares problem is equivalent to \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\operatorname{tr}(A^\\top A X)-2b^\\top Ax+b^\\top b \\\\ \n",
    "\\text{subject to} & X = xx^\\top\\\\\n",
    "& X_{ii} = 1,\\quad i=1,\\dots,n\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Where we consider the minimization problem with variables $x\\in \\mathbb R^n$ and $X\\in\\mathbb S(n)$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-migration",
   "metadata": {
    "id": "v8TSwkEQ2kwV"
   },
   "source": [
    "We expand the objective of the Boolean least squares problem, \n",
    "$$\n",
    "\\|Ax-b\\|^2 = x^\\top A^\\top Ax-2b^\\top Ax+b^\\top b\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "    x^\\top A^\\top A x = \\operatorname{tr}(x^\\top A^\\top A x)=\\operatorname{tr}( A^\\top A xx^\\top)\n",
    "$$\n",
    "\n",
    "Under the constraint $X=xx^\\top$ we thus get an equivalence of the objectives of the two problems. Since $(xx^\\top)_{ii}=x_i^2$, and $x_i^2=1$ if and only if $x_i\\in\\{-1,1\\}$, we conclude the problems are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-daisy",
   "metadata": {
    "id": "unlimited-chile"
   },
   "source": [
    "### Exercise 3b)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "We want to write the objective function of 3a) as an SDP objective of form $\\operatorname{tr}(BY)$. To this end, let \n",
    "$$\n",
    "Y = \\begin{pmatrix}\n",
    "X & x \\\\ \n",
    "x^\\top & 1\n",
    "\\end{pmatrix},\\qquad B=\\begin{pmatrix}C&d\\\\d^\\top &\\alpha\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Find the symmetric block matrix $B$ such that\n",
    "$$\n",
    "\\operatorname{tr}(A^\\top A X)-2b^\\top Ax+b^\\top b = \\operatorname{tr}(BY)\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-threat",
   "metadata": {
    "id": "5M9rbdH96zF3"
   },
   "source": [
    "We compute\n",
    "$$\n",
    "BY = \\begin{pmatrix}X & x \\\\ x^\\top & 1\\end{pmatrix}\\begin{pmatrix}C&d\\\\d^\\top &\\alpha\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "CX+d^\\top x & cx+d \\\\ d^\\top X+\\alpha x^\\top & d^\\top x+\\alpha\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "$$\n",
    "\\operatorname{tr}(BY) = \\operatorname{tr}(CX) + 2d^\\top x+ \\alpha\n",
    "$$\n",
    "\n",
    "giving\n",
    "$$\n",
    "C = A^\\top A,\\quad d=-b^\\top A,\\quad \\alpha=b^\\top b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-needle",
   "metadata": {
    "id": "suspected-tamil"
   },
   "source": [
    "### Exercise 3c)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $B$ be as in the previous exercise. Show that the following SDP is a convex relaxation of the Boolean least squares problem, i.e. the solution gives a lower bound to the Boolean least squares problem:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\operatorname{tr}(BY) \\\\\n",
    "\\text{subject to} & Y\\succeq0\\\\\n",
    "& Y_{ii}=1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-supervision",
   "metadata": {
    "id": "qjL1mgpB93iA"
   },
   "source": [
    "If we set\n",
    "$$\n",
    "Y = \\begin{pmatrix}xx^\\top & x\\\\x^\\top & 1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Then the objective reduces to that of the Boolean least squares problem on the feasible set by 3a) and 3b). Furthermore if $x$ is feasible for the Boolean least squares problem, then $x_i^2 = (xx^\\top)_{ii}=1$ and hence $Y_{ii}=1$. Furthermore $Y$ is positive semidefinite. Indeed, if $\\xi=(y,\\alpha)$ with $y\\in \\mathbb R^n$ and $\\alpha\\in \\mathbb R$ then $\\xi^\\top Y\\xi = (y^\\top x+\\alpha)^2\\geq 0$. Therefore the feasible set for the SDP is larger than that for the Boolean least squares problem, and the SDP is thus a relaxation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
