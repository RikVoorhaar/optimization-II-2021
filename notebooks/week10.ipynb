{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interesting-pittsburgh",
   "metadata": {
    "id": "contrary-capacity"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RikVoorhaar/optimization-II-2021/blob/master/notebooks/week10.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-angola",
   "metadata": {
    "id": "educated-dodge"
   },
   "source": [
    "# Week 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-toner",
   "metadata": {
    "id": "perceived-occasions"
   },
   "source": [
    "## Exercise 1\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Consider the following convex problem\n",
    "$$\n",
    "\t\\begin{array}{ll}\n",
    "\t\\mbox{minimize} & f_0(x)\\\\\n",
    "\t\\mbox{subject to} & f_i(x)\\leq0\\,,\\quad i=1,\\ldots,m\\,.\n",
    "\t\\end{array}\n",
    "$$\n",
    "    \n",
    "where all $f_i$ are differentiable. Assume that $x^*\\in\\mathbb{R}^n$ and $\\lambda^*\\in\\mathbb{R}^m$ satisfy the KKT conditions. Prove by using properties of convex functions and the KKT conditions that\n",
    "$$\n",
    "\t\\nabla f_0(x^*)^T(x-x^*)\\geq0\n",
    "$$\n",
    "\n",
    "for all feasible $x$. (We have seen a similar result when $x^*$ is the global minimum. Here you need to prove it for a point that satisfies the KKT conditions.)\n",
    "\n",
    "_Hint: Show that_ $(x^*,\\lambda^*)$ _satisfies_\n",
    "$$\\sum_{i=1}^m \\lambda_i^*(f_i(x^*)+\\nabla f_i(x^*)^\\top (x-x^*)) = -\\nabla f_0(x^*)^\\top (x-x^*)$$\n",
    "\n",
    "_for any_ $x$_, and use feasibility of_ $x$ _to prove that the left-hand side is negative._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-survival",
   "metadata": {
    "id": "still-trail"
   },
   "source": [
    "## Exercise 2: Inequalities for steepest descent\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "In this exercise we will derive some inequalities that will be important for the analysis of gradient descent.\n",
    "\n",
    "Recall that for $\\beta>0$ a function $f\\colon \\mathbb R^n\\to \\mathbb R$ is called $\\beta$-smooth if $f$ is differentiable and for all $x,y\\in \\mathbb R^n$ we have the following inequality:\n",
    "$$\n",
    "    \\| \\nabla f(x) - \\nabla f(y)\\|\\leq \\beta\\|x-y\\|\n",
    "$$\n",
    "    \n",
    "(in other words, the gradient is $\\beta$-Lipschitz).\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-netherlands",
   "metadata": {
    "id": "exposed-hampton"
   },
   "source": [
    "### Exercise 2a)\n",
    "\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $f$ be $\\beta$-smooth, show that for all $x,y\\in \\mathbb R^n$ we have \n",
    "$$\n",
    "    |f(x)-f(y)-\\nabla f(y)^\\top (x-y)|\\leq \\frac\\beta2\\|x-y\\|^2\n",
    "$$\n",
    "\n",
    "_Hint: Write_ $f(x)-f(y)$ _as_ \n",
    "$$\n",
    "\\int_0^1 \\frac{d}{dt} f(y+t(x-y))\\,\\mathrm dt= \\int_0^1\\nabla f(y+t(x-y))^\\top (x-y)\\,\\mathrm dt,\n",
    "$$\n",
    "    \n",
    "_Use this to write the entire expression under the absolute value signs as an integral over an inner product_ $\\int_0^1 a(t)^\\top b(t)\\,\\mathrm dt$_, and apply Cauchy-Schwarz to this inner product._\n",
    "\n",
    "_Remark: If we also assume that_ $f$ _is convex, then we can drop the absolute value sign, since this expression is always positive for convex functions._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-timber",
   "metadata": {
    "id": "infectious-indie"
   },
   "source": [
    "### Exercise 2b)\n",
    "\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Assume $f$ is convex and $\\beta$-smooth. Show that for all $x,y\\in\\mathbb R^n$ we have\n",
    "$$\n",
    "    f(x)-f(y)\\leq \\nabla f(x)^\\top (x-y)-\\frac1{2\\beta} \\|\\nabla f(y)-\\nabla f(x)\\|^2\n",
    "$$\n",
    "\n",
    "_Hint: define_ $z=y-\\frac1\\beta (\\nabla f(y)-\\nabla f(x))$ _and consider_ \n",
    "$$f(x)-f(y) = (f(x)-f(z))+(f(z)-f(y)).\n",
    "$$\n",
    "    \n",
    "_Use convexity to bound the first term, and use the inequality of 2a) to bound the second term._\n",
    "\n",
    "_Remark: We only need_ $\\beta$_-smoothness here to use the inequality of 2a), so if_ $f$ _is convex and satisfies the inequality of 2a), it also satisfies the inequality of this exercise. This observation will be useful in 2d)._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-baker",
   "metadata": {
    "id": "civic-socket"
   },
   "source": [
    "### Exercise 2c)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Prove the converse of 2b), that is, prove that if $f:\\mathbb R^n\\to \\mathbb R$ is differentiable and if for all $x,y\\in\\mathbb R^n$ we have \n",
    "$$\n",
    "    f(x)-f(y)\\leq \\nabla f(x)^\\top (x-y)-\\frac1{2\\beta} \\|\\nabla f(y)-\\nabla f(x)\\|^2\n",
    "$$\n",
    "\n",
    "then $f$ is convex and $\\beta$-smooth. To show $\\beta$-smoothenss, use the inequality of 2b) to show the stronger result that\n",
    "$$\n",
    "\\|\\nabla f(y)-\\nabla f(x)\\|^2\\leq  \\beta (\\nabla f(x)-\\nabla f(y))^\\top (x-y)\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-garage",
   "metadata": {
    "id": "twenty-comment"
   },
   "source": [
    "### Exercise 2d)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Suppose that $f\\colon \\mathbb R^n\\to \\mathbb R$ is $\\alpha$-strongly convex, and $\\beta$-smooth. Show that for all $x,y\\in\\mathbb R^n$ we have\n",
    "$$\n",
    "    (\\nabla f(x)-\\nabla f(y))^\\top (x-y) \\geq \\frac{\\alpha\\beta}{\\beta+\\alpha}\\|x-y\\|^2 +\\frac{1}{\\beta-\\alpha}\\|\\nabla f(x) - \\nabla f(y)\\|^2\n",
    "$$\n",
    "\n",
    "This result is useful for improving an easier convergence result of gradient descent. \n",
    "\n",
    "_Hint: Use_ $\\alpha$_-strong convexity to show that_ $\\phi(x):=f(x)-\\frac\\alpha2 \\|x\\|^2$ _is convex and_ $(\\beta-\\alpha)$_-smooth. To show_ $(\\beta-\\alpha)$_-smoothness, rewrite the inequality of 2a) in terms of_ $\\phi$. _Then finally apply the stronger inequality mentioned in 2c) to_ $\\phi$ _to obtain the required result after some algebraic manipulation._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-manufacturer",
   "metadata": {
    "id": "incoming-monroe"
   },
   "source": [
    "## Exercise 3: Boolean least squares\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $A\\in \\mathbb R^{m\\times n}$ and $b\\in \\mathbb R^m$. We consider the Boolean least squares problem\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    \\text{minimize} & \\|Ax-b\\|^2\\\\\n",
    "    \\text{subject to} & x_i\\in \\{-1,\\,1\\}, \\quad i=1,\\dots,n\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This is not a convex problem, and we thus want to relax it to a convex problem giving a useful lower bound.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-spirituality",
   "metadata": {
    "id": "atomic-mexico"
   },
   "source": [
    "### Exercise 3a)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Show that the Boolean least squares problem is equivalent to \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\operatorname{tr}(A^\\top A X)-2b^\\top Ax+b^\\top b \\\\ \n",
    "\\text{subject to} & X = xx^\\top\\\\\n",
    "& X_{ii} = 1,\\quad i=1,\\dots,n\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Where we consider the minimization problem with variables $x\\in \\mathbb R^n$ and $X\\in\\mathbb S(n)$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-think",
   "metadata": {
    "id": "unlimited-chile"
   },
   "source": [
    "### Exercise 3b)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "We want to write the objective function of 3a) as an SDP objective of form $\\operatorname{tr}(BY)$. To this end, let \n",
    "$$\n",
    "Y = \\begin{pmatrix}\n",
    "X & x \\\\ \n",
    "x^\\top & 1\n",
    "\\end{pmatrix},\\qquad B=\\begin{pmatrix}C&d\\\\d^\\top &\\alpha\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Find the symmetric block matrix $B$ such that\n",
    "$$\n",
    "\\operatorname{tr}(A^\\top A X)-2b^\\top Ax+b^\\top b = \\operatorname{tr}(BY)\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-recipe",
   "metadata": {
    "id": "suspected-tamil"
   },
   "source": [
    "### Exercise 3c)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $B$ be as in the previous exercise. Show that the following SDP is a convex relaxation of the Boolean least squares problem, i.e. the solution gives a lower bound to the Boolean least squares problem:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\operatorname{tr}(BY) \\\\\n",
    "\\text{subject to} & Y\\succeq0\\\\\n",
    "& Y_{ii}=1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}