{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "contrary-capacity"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RikVoorhaar/optimization-II-2021/blob/master/notebooks/week13.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "educated-dodge"
   },
   "source": [
    "# Week 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "perceived-occasions"
   },
   "source": [
    "## Exercise 1: Supporting hyperplane Theorem\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "In this exercise we will prove the supporting hyperplane theorem:\n",
    "\n",
    "Let $C\\subset \\mathbb R^n$ be a closed convex set. Suppose $x\\in  \\mathbb R^n\\setminus \\operatorname{int}(C)=( \\mathbb R^n\\setminus C)\\cup \\partial C$, then there exists a vector $a\\in  \\mathbb R^n$ such that \n",
    "$$\n",
    "    a^\\top y \\leq a^\\top x\\qquad \\forall x\\in C\n",
    "$$\n",
    "\n",
    "That is, $C$ is on one side of the hyperplane that goes through $x$ and that is orthogonal to $a$.\n",
    "\n",
    "_Remark: the theorem also holds if_ $C$ _is not closed. This can essentially be proved by replacing_ $C$ _with its closure_ $\\overline C$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnIaoDbc27lG"
   },
   "source": [
    "### Exercise 1a)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $x\\in \\mathbb R^n\\setminus C$ (so we exclude the case $x\\in\\partial C$). Using the fact that $C$ is closed we can define the _projection onto_ $C$ by\n",
    "$$\n",
    "    P_C(x) = \\operatorname{argmin}_{y\\in C}\\frac12\\|x-y\\|^2.\n",
    "$$\n",
    "\n",
    "Use the optimality conditions from convex optimization to show that \n",
    "$$\n",
    "a = \\frac{x-P_C(x)}{\\|x-P_C(x)\\|}\n",
    "$$\n",
    "\n",
    "satisfies\n",
    "$$\n",
    "a^\\top y<a^\\top x,\\qquad \\forall y\\in C.\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJCM1LtU4L58"
   },
   "source": [
    "Recall that if $x^*=\\operatorname{argmin}_xf(x)$, then the optimality condition is $\\nabla f^\\top (x^*)(y-x^*)\\geq 0$ for all $y\\in C$. For the projection this means that \n",
    "$$\n",
    "(P_C(x)-x)^\\top(y-P_C(x))\\geq 0\n",
    "$$\n",
    "\n",
    "and thus that\n",
    "$$\n",
    "a^\\top y\\leq a^\\top P_C(x)=a^\\top (P_C(x)-x)+a^\\top x<a^\\top x\n",
    "$$\n",
    "\n",
    "since $a^\\top (P_C(x)-x) = -\\|x-P_C(x)\\| <0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfsLvyah_Prz"
   },
   "source": [
    "### Exercise 1b)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Now consider the case that $x\\in \\partial C$, but $C$ still closed. Take a sequence $(x_k)$ with $x_k\\notin C$ such that $\\lim_k x_k=x$. Then consider\n",
    "$$\n",
    "a_k = \\frac{x_k-P_C(x_k)}{\\|x_k-P_C(x_k)\\|}.\n",
    "$$\n",
    "\n",
    "Prove that $(a_k)$ has a limit point $a$ satisfying the requirements of the supporting the hyperplane theorem.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl7vwDJKypjx"
   },
   "source": [
    "The sequence $(a_k)$ is bounded, and hence has a limit point $a$. By the same reasoning as before we then have,\n",
    "$$\n",
    "a_k^\\top y< a_k^\\top x_k,\\qquad \\forall y\\in C\n",
    "$$\n",
    "\n",
    "Hence if we restrict to a subsequence of $a_k$ converging to $a$, then in the limit we obtain\n",
    "$$\n",
    "a^\\top y\\leq a^\\top x,\\qquad \\forall y\\in C,\n",
    "$$\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrvcGpK0w8Cm"
   },
   "source": [
    "## Exercise 2: Heavy ball method\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Last week we studied the converge rate of gradient descent for unconstrained optimization of the quadratic function\n",
    "$$\n",
    "    f(x) = \\frac12x^\\top A x -b^\\top x+c.\n",
    "$$\n",
    "\n",
    "This time we will see a different method, with asymptotically optimal convergence rate when applied to $f$. In particular, we consider the \"heavy ball\" iteration (Boris Polyak, 1964)\n",
    "$$\n",
    "x_{k+1}=x_k - \\alpha \\nabla f(x_k)+\\beta(x_k-x_{k-1}).\n",
    "$$\n",
    "\n",
    "We will analyse the convergence speed of this method.\n",
    "\n",
    "_Remark: The term_ $x_k-x_{k-1}$ _is refered to as \"momentum\", since it increases the step size proportionally to the last stepsize._\n",
    "\n",
    "_The heavy ball iteration is equivalent to a discretization of the ODE_\n",
    "$$\n",
    "\\ddot{x}+a\\dot{x} +b\\nabla f(x) = 0\n",
    "$$\n",
    "\n",
    "_which models the motion of a particle in potential field_ $f$ _with a friction term, hence the name \"heavy ball\", since it could be use to model a heavy ball moving through a fluid, for example. This term will damp out the oscillations that occur in the steepest descent method and should allow the iteration to converge faster._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T6QApMww8Lt"
   },
   "source": [
    "### Exercise 2a)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $x_*$ be the solution to the optimization problem. Since $x_{k+1}$ depends on both $x_k$ and $x_{k-1}$ we will have to study the convergence rate two steps at a time. To do this we want to find a matrix $T$ \n",
    "$$\n",
    "    \\begin{pmatrix}x_{k+1}-x_*\\\\x_k-x_*\\end{pmatrix} = T\\begin{pmatrix}x_{k}-x_*\\\\x_{k-1}-x_*\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Find such a matrix $T$ of form\n",
    "$$\n",
    "T=\\begin{pmatrix}c_1I+c_2A&c_3I\\\\c_4I&0\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "_Hint: like last week, note that_ $Ax_*=b$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMjfEZ_GS_8A"
   },
   "source": [
    "First of all it's easy to see that $c_4=1$. For the other three we get equation\n",
    "$$\n",
    "x_{k+1}-x_* = c_1(x_k-x_*)+c_2(Ax_k-c_2Ax_*)+c_3(x_{k-1}-x_*)\n",
    "$$\n",
    "\n",
    "The left hand side is given by\n",
    "$$\n",
    "x_k - \\alpha \\nabla f(x_k)+\\beta(x_k-x_{k-1})=x_k - \\alpha (Ax_k-b)+\\beta(x_k-x_{k-1})\n",
    "$$\n",
    "\n",
    "Using the fact that $Ax_*=b$ this gives \n",
    "$$\n",
    "\\begin{align}\n",
    "    1+\\beta &=c_1 \\\\\n",
    "    -\\alpha &= c_2\\\\\n",
    "    1 &= c_1+c_3 \\Rightarrow c_3 = -\\beta\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus\n",
    "$$\n",
    "T=\\begin{pmatrix}(1+\\beta)I-\\alpha A&-\\beta I\\\\I&0\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFdMZoBiw8PG"
   },
   "source": [
    "### Exercise 2b)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $U AU^\\top=\\Lambda=\\operatorname{diag}(\\lambda_1,\\dots,\\lambda_n)$ be an eigendecomposition of $A$. \n",
    "\n",
    "Show that there is a [permutation matrix](https://en.wikipedia.org/wiki/Permutation_matrix) $\\Pi$ such that\n",
    "$$\n",
    "\\Pi\\begin{pmatrix}U&0\\\\0&U\\end{pmatrix}T\\begin{pmatrix}U^\\top&0\\\\0&U^\\top\\end{pmatrix}\\Pi^\\top = \\begin{pmatrix}T_1&0&\\cdots&0\\\\ 0 & T_2&\\cdots &0\\\\\\vdots &\\vdots&\\ddots &\\vdots \\\\ 0&0&\\cdots &T_n\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "with $T_i$ a $2\\times 2$ block matrix\n",
    "$$\n",
    "T_i = \\begin{pmatrix}c_1+c_2\\lambda_i&c_3\\\\c_4&0\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7ZeNHziW85b"
   },
   "source": [
    "We have that\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\begin{pmatrix}U&0\\\\0&U\\end{pmatrix}\\begin{pmatrix}(1+\\beta)I-\\alpha A&-\\beta I\\\\I&0\\end{pmatrix}\\begin{pmatrix}U^\\top&0\\\\0&U^\\top\\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}(1+\\beta)I-\\alpha \\Lambda&-\\beta I\\\\I&0\\end{pmatrix}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This can be conjugated by a permutation matrix to obtain a $2\\times 2$ block diagonal matrix by the shuffle permutation\n",
    "$$\n",
    "\\Pi_{i,j} = (e_1,e_{n+1},e_2,e_{n+2},\\dots,e_n,e_{2n})\n",
    "$$\n",
    "\n",
    "We simply need to send the top-left diagonal to the odd-diagonal entries of the new matrix, the bottom-left diagonal to the even-diagonal elements. This already fixes the permutation, and it does precisely the right thing to the off-diagonal blocks. We can see it in action for small matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1621883183643,
     "user": {
      "displayName": "Rik Voorhaar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiARLX-61EWkT5u3TJ0aLe8HCXUEuzsxfIptxxWrw=s64",
      "userId": "13522764759478134685"
     },
     "user_tz": -120
    },
    "id": "SxpRciuThji0",
    "outputId": "07f3e79f-f01d-42ec-d75f-1b0086f27e37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi = \n",
      "[[1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1]]\n",
      "X = \n",
      "[[1 0 0 4 0 0]\n",
      " [0 2 0 0 5 0]\n",
      " [0 0 3 0 0 6]\n",
      " [7 0 0 0 0 0]\n",
      " [0 8 0 0 0 0]\n",
      " [0 0 9 0 0 0]]\n",
      "Pi@X@Pi.T = \n",
      "[[1 4 0 0 0 0]\n",
      " [7 0 0 0 0 0]\n",
      " [0 0 2 5 0 0]\n",
      " [0 0 8 0 0 0]\n",
      " [0 0 0 0 3 6]\n",
      " [0 0 0 0 9 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 3\n",
    "Pi = np.zeros((2*n,2*n),dtype=int)\n",
    "for i in range(n):\n",
    "    Pi[i,2*i] = 1\n",
    "    Pi[i+n,2*i+1]=1\n",
    "print(\"Pi = \")\n",
    "print(Pi)\n",
    "\n",
    "A = np.zeros((2*n,2*n),dtype=int)\n",
    "for i in range(n):\n",
    "    A[i,i] = i+1\n",
    "    A[i,i+n]=i+n+1\n",
    "    A[i+n,i] = i+2*n+1\n",
    "print(\"X = \")\n",
    "print(A)\n",
    "print(\"Pi@X@Pi.T = \")\n",
    "print(Pi.T@A@Pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d5qOTZRw8Sa"
   },
   "source": [
    "### Exercise 2c)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "The eigenvalues of each individual block $T_i$ are given by the roots of the equation\n",
    "$$\n",
    "u^2-(1+\\beta-\\alpha\\lambda_i)u+\\beta=0.\n",
    "$$\n",
    "\n",
    "Show that if $(1+\\beta-\\alpha\\lambda_i)^2\\leq 4\\beta$ then the roots are complex and have magnitude\n",
    "$$\n",
    "|u|^2 = \\beta.\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7nRvWfYxWFM"
   },
   "source": [
    "The block $T_i$ is given by \n",
    "$$\n",
    "\\begin{pmatrix}1+\\beta-\\alpha\\lambda_i&-\\beta\\\\1&0\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "hence the characteristic equation is indeed $u^2-(1+\\beta-\\alpha\\lambda_i)u+\\beta=0$. The roots of this equation are\n",
    "$$\n",
    "    u=\\frac12\\left(1+\\beta-\\alpha\\lambda\\pm \\sqrt{(1+\\beta-\\alpha\\lambda)^2-4\\beta}\\right).\n",
    "$$\n",
    "\n",
    "Hence, if $(1+\\beta-\\alpha\\lambda_i)^2\\leq 4\\beta$, the discriminant is negative. The magnitude is then the same for both roots, and given by\n",
    "$$\n",
    "|u|^2 = \\frac14\\left((1+\\beta-\\alpha\\lambda)^2+4\\beta-(1+\\beta-\\alpha\\lambda)^2\\right) = \\beta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvrYbee3w8Vm"
   },
   "source": [
    "### Exercise 2d)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $\\lambda_1$ and $\\lambda_n$ be respectively the smallest and largest eigenvalue of $A$.\n",
    "Show that if we choose $\\beta = \\max\\{(1-\\sqrt{\\alpha\\lambda_1})^2,(1-\\sqrt{\\alpha \\lambda_n})^2\\}$, then the condition $(1+\\beta-\\alpha\\lambda_i)^2\\leq 4\\beta$ is satisfied for each $i$. Conclude that then the [spectral radius](https://en.wikipedia.org/wiki/Spectral_radius) of $T$ is given by $\\rho(T)=\\sqrt\\beta$. \n",
    "\n",
    "_Hint: Prove that_ $(1+\\beta-\\alpha\\lambda_i)^2\\leq 4\\beta$ _is equivalent to_ $(1-\\sqrt{\\alpha \\lambda_i})^2\\leq\\beta\\leq (1+\\sqrt{\\alpha\\lambda_i})^2$. _For this it may be easier to first solve the equation_ $(1+\\beta-\\alpha\\lambda_i)^2= 4\\beta$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aY_FnIeq2NHY"
   },
   "source": [
    "We consider the inequality $(1+\\beta-\\alpha\\lambda_i)^2\\leq 4\\beta$. This is an inequality where a (positive) parabola is smaller than a line, which happens between the two intersection points (if they exist). Therefore we solve\n",
    "$(1+\\beta-\\alpha\\lambda_i)^2=4\\beta$. Expanding the left hand side gives equation\n",
    "$$\n",
    "\\beta^2+\\beta(-2-2\\lambda_i)+(1-\\alpha\\lambda_i)^2 = 0\n",
    "$$\n",
    "\n",
    "Solving this gives\n",
    "$$\n",
    "    \\beta = 1+\\alpha\\lambda_i\\pm2\\sqrt{\\alpha\\lambda_i}\n",
    "$$\n",
    "\n",
    "which is equivalent to $\\beta\\in \\{(1-\\sqrt{\\alpha\\lambda_i})^2,(1+\\sqrt{\\alpha\\lambda_i})^2\\}$. Therefore $(1+\\beta-\\alpha\\lambda_i)^2\\leq 4\\beta$ is equivalent to $(1-\\sqrt{\\alpha \\lambda_i})^2\\leq\\beta\\leq (1+\\sqrt{\\alpha\\lambda_i})^2$.\n",
    "\n",
    "Now if $\\beta=\\max\\{(1-\\sqrt{\\alpha\\lambda_1})^2,(1-\\sqrt{\\alpha \\lambda_n})^2\\}$ then $1\\geq \\beta\\geq (1-\\sqrt{\\alpha\\lambda_i})^2$ for all $\\lambda_i$. By the previous exercise, the eigenvalues of all blocks $T_i$ have therefore magnitude $\\sqrt{\\beta}$, and thus every eigenvalue lies on the cirlce of radius $\\sqrt{\\beta}$. Therefore $\\rho(T)=\\sqrt{\\beta}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXRjlfu_w8Y3"
   },
   "source": [
    "### Exercise 2e)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "\n",
    "To get fast convergence, we want to minimize $\\rho(T)$. Find the value of $\\alpha>0$ that minimizes the spectral radius\n",
    "$\\rho(T)^2=\\beta=\\max\\{(1-\\sqrt{\\alpha\\lambda_1})^2,(1-\\sqrt{\\alpha \\lambda_n})\\}$. Show that then \n",
    "$$\n",
    "\\rho(T) = \\frac{\\sqrt{\\lambda_n}-\\sqrt{\\lambda_1}}{\\sqrt{\\lambda_n}+\\sqrt{\\lambda_1}}\n",
    "$$\n",
    "\n",
    "_Hint: this is very similar to exercise 2c) of week 12_\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UcEYrPn-Nc2"
   },
   "source": [
    "Like in exercise 2c) of week 12, we can argue that the max of these two functions is minimized precisely if \n",
    "$$\n",
    "(1-\\sqrt{\\alpha\\lambda_1})=-(1-\\sqrt{\\alpha \\lambda_n})\n",
    "$$\n",
    "\n",
    "This is equivalent to \n",
    "$$\n",
    "\\alpha = \\frac{4}{(\\sqrt\\lambda_n+\\sqrt\\lambda_1)^2}\n",
    "$$\n",
    "\n",
    "Plugging this in for $\\beta$ gives\n",
    "$$\n",
    "\\beta = \\left(\\frac{\\sqrt{\\lambda_n}-\\sqrt{\\lambda_1}}{\\sqrt{\\lambda_n}+\\sqrt{\\lambda_1}}\\right)^2\n",
    "$$\n",
    "\n",
    "using $\\rho(T)=\\sqrt{\\beta}$ gives the required result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-JjBCxqw8cM"
   },
   "source": [
    "### Exercise 2f)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Recall [Gelfand's formula for the spectral radius](https://en.wikipedia.org/wiki/Spectral_radius#Gelfand's_formula) that tells us that\n",
    "$$\n",
    "\\rho(T) = \\lim_{k\\to\\infty}\\|T^k\\|^{\\frac1k}.\n",
    "$$\n",
    "\n",
    "Therefore, for any $\\epsilon>0$ we have for all $k$ sufficiently large that\n",
    "$$\n",
    "\\|T^k\\|\\leq (\\rho(T)+\\epsilon)^k.\n",
    "$$\n",
    "\n",
    "Let $\\kappa=\\lambda_n/\\lambda_1$ be the condition number of $A$. Show that for a good choice of $\\alpha,\\beta$ we have for all $\\epsilon>0$ and $k$ sufficiently large that\n",
    "$$\n",
    "\\left\\|\\begin{matrix}x_{k+1}-x_*\\\\x_k-x_*\\end{matrix}\\right\\| \\leq \\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}+\\epsilon\\right)^k\\left\\|\\begin{matrix}x_{1}-x_*\\\\x_{0}-x_*\\end{matrix}\\right\\|.\n",
    "$$\n",
    "\n",
    "Up to the arbitrarily small term $\\varepsilon$, this convergence rate matches the optimal rate that we have seen in the course.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXcr6ST28DSf"
   },
   "source": [
    "Firs of all note simply that\n",
    "$$\n",
    "\\rho(T) = \\frac{\\sqrt{\\lambda_n}-\\sqrt{\\lambda_1}}{\\sqrt{\\lambda_n}+\\sqrt{\\lambda_1}}=\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\n",
    "$$\n",
    "\n",
    "From exercise 2a) we have that\n",
    "$$\n",
    "\\begin{pmatrix}x_{k+1}-x_*\\\\x_k-x_*\\end{pmatrix} = T\\begin{pmatrix}x_{k}-x_*\\\\x_{k-1}-x_*\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Hence \n",
    "$$\n",
    "\\left\\|\\begin{matrix}x_{k+1}-x_*\\\\x_k-x_*\\end{matrix}\\right\\| \\leq \\|T\\|\\left\\|\\begin{matrix}x_{k}-x_*\\\\x_{k-1}-x_*\\end{matrix}\\right\\|\n",
    "$$\n",
    "\n",
    "By induction\n",
    "$$\n",
    "\\left\\|\\begin{matrix}x_{k+1}-x_*\\\\x_k-x_*\\end{matrix}\\right\\| \\leq \\|T\\|^k\\left\\|\\begin{matrix}x_{1}-x_*\\\\x_{0}-x_*\\end{matrix}\\right\\|\n",
    "$$\n",
    "\n",
    "Then using the remark about Gelfand's formula together with the expression for $\\rho(T)$ gives the result."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week13.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('optimizationII': conda)",
   "name": "python388jvsc74a57bd02e856940cf7aa8b7f2d1e92bbcbae8d63329fadf2732f6cb71d2f9eeabdec03b"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}