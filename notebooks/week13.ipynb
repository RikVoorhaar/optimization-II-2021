{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seven-judge",
   "metadata": {
    "id": "contrary-capacity"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RikVoorhaar/optimization-II-2021/blob/master/notebooks/week13.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-composer",
   "metadata": {
    "id": "educated-dodge"
   },
   "source": [
    "# Week 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-calendar",
   "metadata": {
    "id": "perceived-occasions"
   },
   "source": [
    "## Exercise 1: Supporting hyperplane Theorem\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "In this exercise we will prove the supporting hyperplane theorem:\n",
    "\n",
    "Let $C\\subset \\mathbb R^n$ be a closed convex set. Suppose $x\\in  \\mathbb R^n\\setminus \\operatorname{int}(C)=( \\mathbb R^n\\setminus C)\\cup \\partial C$, then there exists a vector $a\\in  \\mathbb R^n$ such that \n",
    "$$\n",
    "    a^\\top y \\leq a^\\top x\\qquad \\forall x\\in C\n",
    "$$\n",
    "\n",
    "That is, $C$ is on one side of the hyperplane that goes through $x$ and that is orthogonal to $a$.\n",
    "\n",
    "_Remark: the theorem also holds if_ $C$ _is not closed. This can essentially be proved by replacing_ $C$ _with its closure_ $\\overline C$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-restoration",
   "metadata": {
    "id": "RnIaoDbc27lG"
   },
   "source": [
    "### Exercise 1a)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $x\\in \\mathbb R^n\\setminus C$ (so we exclude the case $x\\in\\partial C$). Using the fact that $C$ is closed we can define the _projection onto_ $C$ by\n",
    "$$\n",
    "    P_C(x) = \\operatorname{argmin}_{y\\in C}\\frac12\\|x-y\\|^2.\n",
    "$$\n",
    "\n",
    "Use the optimality conditions from convex optimization to show that \n",
    "$$\n",
    "a = \\frac{x-P_C(x)}{\\|x-P_C(x)\\|}\n",
    "$$\n",
    "\n",
    "satisfies\n",
    "$$\n",
    "a^\\top y<a^\\top x,\\qquad \\forall y\\in C.\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-topic",
   "metadata": {
    "id": "DfsLvyah_Prz"
   },
   "source": [
    "### Exercise 1b)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Now consider the case that $x\\in \\partial C$, but $C$ still closed. Take a sequence $(x_k)$ with $x_k\\notin C$ such that $\\lim_k x_k=x$. Then consider\n",
    "$$\n",
    "a_k = \\frac{x_k-P_C(x_k)}{\\|x_k-P_C(x_k)\\|}.\n",
    "$$\n",
    "\n",
    "Prove that $(a_k)$ has a limit point $a$ satisfying the requirements of the supporting the hyperplane theorem.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-watch",
   "metadata": {
    "id": "TrvcGpK0w8Cm"
   },
   "source": [
    "## Exercise 2: Heavy ball method\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Last week we studied the converge rate of gradient descent for unconstrained optimization of the quadratic function\n",
    "$$\n",
    "    f(x) = \\frac12x^\\top A x -b^\\top x+c.\n",
    "$$\n",
    "\n",
    "This time we will see a different method, with asymptotically optimal convergence rate when applied to $f$. In particular, we consider the \"heavy ball\" iteration (Boris Polyak, 1964)\n",
    "$$\n",
    "x_{k+1}=x_k - \\alpha \\nabla f(x_k)+\\beta(x_k-x_{k-1}).\n",
    "$$\n",
    "\n",
    "We will analyse the convergence speed of this method.\n",
    "\n",
    "_Remark: The term_ $x_k-x_{k-1}$ _is refered to as \"momentum\", since it increases the step size proportionally to the last stepsize._\n",
    "\n",
    "_The heavy ball iteration is equivalent to a discretization of the ODE_\n",
    "$$\n",
    "\\ddot{x}+a\\dot{x} +b\\nabla f(x) = 0\n",
    "$$\n",
    "\n",
    "_which models the motion of a particle in potential field_ $f$ _with a friction term, hence the name \"heavy ball\", since it could be use to model a heavy ball moving through a fluid, for example. This term will damp out the oscillations that occur in the steepest descent method and should allow the iteration to converge faster._\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-dryer",
   "metadata": {
    "id": "0T6QApMww8Lt"
   },
   "source": [
    "### Exercise 2a)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $x_*$ be the solution to the optimization problem. Since $x_{k+1}$ depends on both $x_k$ and $x_{k-1}$ we will have to study the convergence rate two steps at a time. To do this we want to find a matrix $T$ \n",
    "$$\n",
    "    \\begin{pmatrix}x_{k+1}-x_*\\\\x_k-x_*\\end{pmatrix} = T\\begin{pmatrix}x_{k}-x_*\\\\x_{k-1}-x_*\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Find such a matrix $T$ of form\n",
    "$$\n",
    "T=\\begin{pmatrix}c_1I+c_2A&c_3I\\\\c_4I&0\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "_Hint: like last week, note that_ $Ax_*=b$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-paraguay",
   "metadata": {
    "id": "rFdMZoBiw8PG"
   },
   "source": [
    "### Exercise 2b)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $U AU^\\top=\\Lambda=\\operatorname{diag}(\\lambda_1,\\dots,\\lambda_n)$ be an eigendecomposition of $A$. \n",
    "\n",
    "Show that there is a [permutation matrix](https://en.wikipedia.org/wiki/Permutation_matrix) $\\Pi$ such that\n",
    "$$\n",
    "\\Pi\\begin{pmatrix}U&0\\\\0&U\\end{pmatrix}T\\begin{pmatrix}U^\\top&0\\\\0&U^\\top\\end{pmatrix}\\Pi^\\top = \\begin{pmatrix}T_1&0&\\cdots&0\\\\ 0 & T_2&\\cdots &0\\\\\\vdots &\\vdots&\\ddots &\\vdots \\\\ 0&0&\\cdots &T_n\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "with $T_i$ a $2\\times 2$ block matrix\n",
    "$$\n",
    "T_i = \\begin{pmatrix}c_1+c_2\\lambda_i&c_3\\\\c_4&0\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-compromise",
   "metadata": {
    "id": "9d5qOTZRw8Sa"
   },
   "source": [
    "### Exercise 2c)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "The eigenvalues of each individual block $T_i$ are given by the roots of the equation\n",
    "$$\n",
    "u^2-(1+\\beta-\\alpha\\lambda_i)u+\\beta=0.\n",
    "$$\n",
    "\n",
    "Show that if $(1+\\beta-\\alpha\\lambda_i)^2\\leq 4\\beta$ then the roots are complex and have magnitude\n",
    "$$\n",
    "|u|^2 = \\beta.\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-donna",
   "metadata": {
    "id": "gvrYbee3w8Vm"
   },
   "source": [
    "### Exercise 2d)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Let $\\lambda_1$ and $\\lambda_n$ be respectively the smallest and largest eigenvalue of $A$.\n",
    "Show that if we choose $\\beta = \\max\\{(1-\\sqrt{\\alpha\\lambda_1})^2,(1-\\sqrt{\\alpha \\lambda_n})^2\\}$, then the condition $(1+\\beta-\\alpha\\lambda_i)^2\\leq 4\\beta$ is satisfied for each $i$. Conclude that then the [spectral radius](https://en.wikipedia.org/wiki/Spectral_radius) of $T$ is given by $\\rho(T)=\\sqrt\\beta$. \n",
    "\n",
    "_Hint: Prove that_ $(1+\\beta-\\alpha\\lambda_i)^2\\leq 4\\beta$ _is equivalent to_ $(1-\\sqrt{\\alpha \\lambda_i})^2\\leq\\beta\\leq (1+\\sqrt{\\alpha\\lambda_i})^2$. _For this it may be easier to first solve the equation_ $(1+\\beta-\\alpha\\lambda_i)^2= 4\\beta$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-multiple",
   "metadata": {
    "id": "mXRjlfu_w8Y3"
   },
   "source": [
    "### Exercise 2e)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "\n",
    "To get fast convergence, we want to minimize $\\rho(T)$. Find the value of $\\alpha>0$ that minimizes the spectral radius\n",
    "$\\rho(T)^2=\\beta=\\max\\{(1-\\sqrt{\\alpha\\lambda_1})^2,(1-\\sqrt{\\alpha \\lambda_n})\\}$. Show that then \n",
    "$$\n",
    "\\rho(T) = \\frac{\\sqrt{\\lambda_n}-\\sqrt{\\lambda_1}}{\\sqrt{\\lambda_n}+\\sqrt{\\lambda_1}}\n",
    "$$\n",
    "\n",
    "_Hint: this is very similar to exercise 2c) of week 12_\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-prevention",
   "metadata": {
    "id": "W-JjBCxqw8cM"
   },
   "source": [
    "### Exercise 2f)\n",
    "<div class=\"alert alert-info\"> Exercise\n",
    "\n",
    "Recall [Gelfand's formula for the spectral radius](https://en.wikipedia.org/wiki/Spectral_radius#Gelfand's_formula) that tells us that\n",
    "$$\n",
    "\\rho(T) = \\lim_{k\\to\\infty}\\|T^k\\|^{\\frac1k}.\n",
    "$$\n",
    "\n",
    "Therefore, for any $\\epsilon>0$ we have for all $k$ sufficiently large that\n",
    "$$\n",
    "\\|T^k\\|\\leq (\\rho(T)+\\epsilon)^k.\n",
    "$$\n",
    "\n",
    "Let $\\kappa=\\lambda_n/\\lambda_1$ be the condition number of $A$. Show that for a good choice of $\\alpha,\\beta$ we have for all $\\epsilon>0$ and $k$ sufficiently large that\n",
    "$$\n",
    "\\left\\|\\begin{matrix}x_{k+1}-x_*\\\\x_k-x_*\\end{matrix}\\right\\| \\leq \\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}+\\epsilon\\right)^k\\left\\|\\begin{matrix}x_{1}-x_*\\\\x_{0}-x_*\\end{matrix}\\right\\|.\n",
    "$$\n",
    "\n",
    "Up to the arbitrarily small term $\\varepsilon$, this convergence rate matches the optimal rate that we have seen in the course.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week13.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
