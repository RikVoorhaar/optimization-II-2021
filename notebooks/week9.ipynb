{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protective-shift",
   "metadata": {
    "id": "fresh-elephant"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RikVoorhaar/optimization-II-2021/blob/master/notebooks/week9.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-brain",
   "metadata": {
    "id": "apart-webmaster"
   },
   "source": [
    "# Week 9\n",
    "\n",
    "This is the fourth Python programming homework for this course. You can do this homework either on Google colab, or on your own machine (after installing conda). Some general instructions for the exercises:\n",
    "\n",
    "- In the description of every exercise we mention which functions you should use, and there will be a link to the relevant documentation. You are strongly encouraged to read the documentation, especially if you are not sure what the function does. \n",
    "\n",
    "- Each exersice consists of finishing some partially written code. The code you wrote is then automatically tested using a few `assert` statements. This is for your convenience, and you are requested __not to alter the tests__. If your code conforms to the instructions, and none of the tests throw an error, your answer is very likely to be correct. \n",
    "\n",
    "- It is possible to solve each exercise in no more than 10-20 lines of code, and most only need 2-3 lines. If your solution is much longer, you should try to make it shorter. In particular, don't forget that using vectorized `numpy` and `scipy` functions tends to produce shorter and much faster code than using nested Python `for` loops. \n",
    "\n",
    "- Before handing in, be sure to restart the IPython kernel (in colab: Runtime $\\to$ Run all) and run the entire notebook top-to-bottom and check that none of the cells throw an error. \n",
    "\n",
    "Now we begin by running the cell below. Google colab by default uses an outdated version of CVXPY, and so we will need to update it. This may take about a minute, but this is only performed once per session. If you're running this notebook on a local python installation, make sure that this cell does not throw any errors. If it does, you will need to install additional packages. You can do this by using `pip` from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "religious-pixel",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63905,
     "status": "ok",
     "timestamp": 1619532155783,
     "user": {
      "displayName": "Rik Voorhaar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiARLX-61EWkT5u3TJ0aLe8HCXUEuzsxfIptxxWrw=s64",
      "userId": "13522764759478134685"
     },
     "user_tz": -120
    },
    "id": "corrected-processor",
    "outputId": "2e6c7300-945a-4317-f0d0-1a0b13049102"
   },
   "outputs": [],
   "source": [
    "from importlib_metadata import version\n",
    "\n",
    "if version('cvxpy')<'1.1.0':\n",
    "   !pip install --upgrade cvxpy\n",
    "\n",
    "import cvxpy as cp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (9, 6)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import sympy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # don't display irrelevant deprecation warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-cancellation",
   "metadata": {
    "id": "pressed-knowing"
   },
   "source": [
    "## Exercise 1: Separating hyperplanes\n",
    "\n",
    "In this series, we will study support vector machines (SVM). We can think of an SVM as a function $f(x)\\colon \\mathbb R^n\\to\\{-1,1\\}$ that satisfies\n",
    "$$\n",
    "f(x) = \\operatorname{sgn}(a^\\top x-b)\n",
    "$$\n",
    "\n",
    "where $a \\in\\mathbb R^n$ and $b\\in\\mathbb R$ are fixed (but learned from data) and $\\operatorname{sgn}$ is the sign function. It has values $1$ on one side of the hyperplane defined by $a^\\top x = b$, and value $-1$ on the other side. \n",
    "\n",
    "A set of points $X=(x_1,\\dots,x_N)\\subset\\mathbb R^n$ and labels $Y=(y_1,\\dots,y_N)\\subset \\{-1,1\\}$ is called _separable_ if there exists a hyperplane that separates the points with label $-1$ from those with label $+1$. In this case, the labels $Y$ can be perfectly modeled using an SVM. Recall from the lectures that we can find a good separating hyperplane (with so-called maximal margin) by solving the following QCQP\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{maximize} & t \\\\\n",
    "\\text{subject to} &y_i(a^\\top x_i-b)\\geq t,\\qquad \\forall i\\\\\n",
    "& \\|a\\|_2^2\\leq 1 \\\\ \n",
    "& t\\geq 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Here the optimal value of $t$ is the minimum distance of any point to the hyperplane. If there is no separating hyperplane, the only feasible point is $a=0,b=0,t=0$. \n",
    "___\n",
    "\n",
    "> Write a function `separating_hyperplane(X,Y)` that finds a separating hyperplane for the points `X` and labels `Y` if it exists. It should also return the minimal distance between the hyperplane and the points. \n",
    "> It should return the data as a tuple `(a,b,t)`, as defined above. If there is no separating hyperplane, then these parameters should all be `None`.  \n",
    "\n",
    "___  \n",
    "\n",
    "Tips:  \n",
    "\n",
    "- In the constraint above, use `cp.multiply` to multiply by `Y`. Using `*` will result in matrix multiplication instead.  \n",
    "\n",
    "- If there is a separating hyperplane, then always $\\|a\\|=1$. If not then $a=0,b=0,t=0$ is the only feasible point, and CVXPY will return a point _close to this_. You can use this to check if a separating hyperplane exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-evolution",
   "metadata": {
    "id": "swedish-original"
   },
   "outputs": [],
   "source": [
    "# First some code for generating separable data,\n",
    "# and plotting the hyperplane in the 2-dimensional case\n",
    "\n",
    "\n",
    "def gen_separable_data(a, b, N=100, separation_factor=1 / 10):\n",
    "    a = np.array(a)\n",
    "    n = len(a)\n",
    "    X = np.random.uniform(low=-1, high=1, size=(n, N))\n",
    "    Y = a @ X > b\n",
    "    X[:, Y] = (X[:, Y].T + a * separation_factor / np.linalg.norm(a)).T\n",
    "    X[:, ~Y] = (X[:, ~Y].T - a * separation_factor / np.linalg.norm(a)).T\n",
    "    Y = 2 * Y.astype(int) - 1\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def plot_separating_hyperplane2D(X, Y, a, b):\n",
    "    # Find two point on the hyperplane (line) far apart\n",
    "    x0 = a * b / np.linalg.norm(a) ** 2  # This point always lies on the line\n",
    "    a_orth = np.array([-a[1], a[0]])  # Direction orthogonal to a\n",
    "    x1 = x0 + 1e6 * a_orth\n",
    "    x2 = x0 - 1e6 * a_orth\n",
    "    plt.plot([x1[0], x2[0]], [x1[1], x2[1]])\n",
    "\n",
    "    # Plot the points so that different labels get different colors\n",
    "    plt.plot(X[0][Y == -1], X[1][Y == -1], \".\")\n",
    "    plt.plot(X[0][Y == 1], X[1][Y == 1], \".\")\n",
    "    plt.ylim(np.min(X[1]) - 0.1, np.max(X[1]) + 0.1)\n",
    "    plt.xlim(np.min(X[0]) - 0.1, np.max(X[0]) + 0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-lodge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 64810,
     "status": "ok",
     "timestamp": 1619532156704,
     "user": {
      "displayName": "Rik Voorhaar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiARLX-61EWkT5u3TJ0aLe8HCXUEuzsxfIptxxWrw=s64",
      "userId": "13522764759478134685"
     },
     "user_tz": -120
    },
    "id": "spread-might",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1107acf2ec9081b94c367371410ab700",
     "grade": true,
     "grade_id": "cell-d95cfcf65fd29ade",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "cfc75386-b6dc-4055-8fbd-e851f4a4928d"
   },
   "outputs": [],
   "source": [
    "def separating_hyperplane(X, Y):\n",
    "    n, N = X.shape\n",
    "\n",
    "    # ENTER YOUR CODE HERE\n",
    "    \n",
    "\n",
    "\n",
    "a_true = np.array([1, 0.4])\n",
    "b_true = -0.5\n",
    "X, Y = gen_separable_data(a_true, b_true, separation_factor=0.1)\n",
    "\n",
    "a, b, t = separating_hyperplane(X, Y)\n",
    "\n",
    "plot_separating_hyperplane2D(X, Y, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-blogger",
   "metadata": {
    "id": "casual-resort"
   },
   "outputs": [],
   "source": [
    "def tests_1():\n",
    "    np.random.seed(179)\n",
    "    for a_true, b_true in (\n",
    "        (np.array([1, 1]), 0.1),\n",
    "        (np.array([1, 2]), 1),\n",
    "        (np.array([1, 1, 1]), 0),\n",
    "    ):\n",
    "        X, Y = gen_separable_data(a_true, b_true, N=1000, separation_factor=1)\n",
    "        a, b, t = separating_hyperplane(X, Y)\n",
    "\n",
    "        def cos_distance(x, y):\n",
    "            return np.abs(np.dot(x, y)) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "        assert (\n",
    "            np.abs(np.min(Y * (a @ X - b)) - t) < 1e-6\n",
    "        ), \"`t` is not the minimal distance to the hyperplane\"\n",
    "        assert (\n",
    "            1 - cos_distance(a, a_true) < 1e-3\n",
    "        ), \"Angle between true hyperplane and computed hyperplane is too big\"\n",
    "    X, Y = gen_separable_data([1, 1], 0, separation_factor=-1)\n",
    "    a, b, t = separating_hyperplane(X, Y)\n",
    "    for var in (a, b, t):\n",
    "        assert (\n",
    "            var is None\n",
    "        ), \"If there is no separating hyperplane, then all variables should be `None`.\"\n",
    "\n",
    "\n",
    "tests_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-defensive",
   "metadata": {
    "id": "legendary-aircraft"
   },
   "source": [
    "## Exercise 2: Robust solution with hinge loss\n",
    "\n",
    "Next, we want to improve the SVM so that it finds 'the best' hyperplane in the case that the data is _not_ separable. We first rewrite the problem in three steps (convince yourself that these problems are all equivalent):\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{maximize} & t \\\\\n",
    "\\text{subject to} &y_i(a^\\top x_i/t-b/t)\\geq 1,\\qquad \\forall i\\\\\n",
    "& \\|a\\|_2^2\\leq 1 \\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{maximize} & t \\\\\n",
    "\\text{subject to} &y_i(a^\\top x_i-b)\\geq 1,\\qquad \\forall i\\\\\n",
    "& \\|a\\|_2^2\\leq 1/t \\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\frac12 \\|a\\|_2^2 \\\\\n",
    "\\text{subject to} &y_i(a^\\top x_i-b)\\geq 1,\\qquad \\forall i\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Now we want to minimize the number of points on the wrong side of the hyperplane. If a point $x_i$ is on the wrong side of the hyperplane then\n",
    "$$\n",
    "    y_i(a^\\top x_i -b) \\leq 1,\\qquad \\Leftrightarrow 1-y_i(a^\\top x_i-b)\\geq 0.\n",
    "$$\n",
    "\n",
    "To penalize only those points on the wrong side of the hyperplane, we can therefore add a _hinge loss_ penalty $\\max(0, 1-y_i(a^\\top x_i-b))$. This gives the unconstrained problem\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\frac12 \\|a\\|_2^2 +C\\sum_{i=1}^N \\max(0, 1-y_i(a^\\top x_i-b))\\\\\n",
    "%\\text{subject to} &y_i(a^\\top x_i-b)\\geq 1,\\qquad \\forall i\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The constant $C>0$ determines how much we penalize misclassification, and we need to vary this depending on the problem that we are solving. The seperable case corresponds to $C=\\infty$ (so no misclassification is allowed since this would give an infinite cost). Notice that $\\xi_i=\\max(0, 1-y_i(a^\\top x_i-b))$ is the smallest number such that $y_i(a^\\top x_i-b)\\geq 1-\\xi_i$. We can therefore rewrite the problem as:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\frac12 \\|a\\|_2^2 +C\\sum_{i=1}^N \\xi_i\\\\\n",
    "\\text{subject to} &y_i(a^\\top x_i-b)\\geq 1-\\xi_i,\\qquad \\forall i\\\\\n",
    "& \\xi_i\\geq 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Here, $\\xi_i=0$ means that $x_i$ lies on the correct side of the hyperplane. If $\\xi>0$ then it is equal to $1-y_i(a^\\top x_i-b)$.\n",
    "\n",
    "___  \n",
    "\n",
    "> Write a function `SVM_primal(X, Y, C=1.0)` that solves this problem, and accepts the value of $C$ as an optional parameter. It should return the optimal value of `(a,b,xi)` in a tuple.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-career",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 64799,
     "status": "ok",
     "timestamp": 1619532156706,
     "user": {
      "displayName": "Rik Voorhaar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiARLX-61EWkT5u3TJ0aLe8HCXUEuzsxfIptxxWrw=s64",
      "userId": "13522764759478134685"
     },
     "user_tz": -120
    },
    "id": "enclosed-huntington",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1b722f0269acbfa1f736c95eac64f12",
     "grade": true,
     "grade_id": "cell-b970b49de091767a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "2e8c4ffd-283d-491e-ced9-4031190e02d7"
   },
   "outputs": [],
   "source": [
    "def SVM_primal(X, Y, C=1.0):\n",
    "    n, N = X.shape\n",
    "\n",
    "    # ENTER YOUR CODE HERE\n",
    "    \n",
    "\n",
    "\n",
    "a_true = np.array([2, 0])\n",
    "b_true = -0.5\n",
    "X, Y = gen_separable_data(a_true, b_true, separation_factor=-0.2)\n",
    "\n",
    "a, b, xi = SVM_primal(X, Y, C=50)\n",
    "\n",
    "plot_separating_hyperplane2D(X, Y, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-exhibit",
   "metadata": {
    "id": "color-comedy"
   },
   "outputs": [],
   "source": [
    "def tests_2():\n",
    "    np.random.seed(179)\n",
    "    for C in [10, 100]:\n",
    "        for a_true, b_true in (\n",
    "            (np.array([1, 1]), 0.1),\n",
    "            (np.array([1, 2]), 1),\n",
    "            (np.array([1, 1, 1]), 0),\n",
    "        ):\n",
    "            X, Y = gen_separable_data(a_true, b_true, N=1000, separation_factor=-0.1)\n",
    "            a, b, xi = SVM_primal(X, Y, C=C)\n",
    "\n",
    "            def cos_distance(x, y):\n",
    "                return np.abs(np.dot(x, y)) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "            assert (\n",
    "                1 - cos_distance(a, a_true) < 1e-3\n",
    "            ), \"Angle between true hyperplane and computed hyperplane is too big\"\n",
    "\n",
    "            assert np.all(\n",
    "                Y * (a @ X - b) >= 1 - 1e4 - xi\n",
    "            ), \"The constraint is not satisfied\"\n",
    "\n",
    "\n",
    "tests_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-chapter",
   "metadata": {
    "id": "drawn-surveillance"
   },
   "source": [
    "## Exercise 3: The dual problem\n",
    "\n",
    "For SVM it turns out that the dual problem is a QP. In general, QPs are easier to solve than SOCPs, like the primal problem. Recall that the primal problem is\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\frac12 \\|a\\|_2^2 +C\\sum_{i=1}^N \\xi_i\\\\\n",
    "\\text{subject to} &y_i(a^\\top x_i-b)\\geq 1-\\xi_i,\\qquad \\forall i\\\\\n",
    "& \\xi_i\\geq 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The Lagrangian is therefore\n",
    "$$\n",
    "L(a,b,\\xi,\\mu,\\nu) = \\frac 12 \\|a\\|^2 + C\\sum_{i=1}^N\\xi_i - \\sum_{i=1}^N \\mu_i\\left(y_i(a^\\top x_i-b) - (1-\\xi_i)\\right) -\\sum_{i=1}^N\\nu_i\\xi_i.\n",
    "$$\n",
    "\n",
    "Let's rewrite this to\n",
    "$$\n",
    "   L(a,b,\\xi,\\mu,\\nu) =\\frac 12 \\|a\\|^2 - \\sum_{i=1}^N \\mu_iy_i(a^\\top x_i)+\\sum_{i=1}^N \\mu_iy_ib+ \\sum_{i=1}^N\\xi_i(C-\\mu_i-\\nu_i)  + \\sum_{i=1}^N\\mu_i.\n",
    "$$\n",
    "\n",
    "We compute the partial derivatives with respect to $a,b,\\xi$ and set them to zero.  \n",
    "\n",
    "- $\\frac{\\partial L}{\\partial a}=0$ gives $a=\\sum_i\\mu_i y_ix_i$.  \n",
    "\n",
    "- $\\frac{\\partial L}{\\partial b}=0$ gives $\\sum_i\\mu_iy_i=0$, this kills the term depending on $b$.\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial \\xi_i}=0$ gives $C-\\mu_i-\\nu_i=0$ for all $i$, this kills the term depending on $\\xi$. \n",
    "\n",
    "- If $\\mu_i<0$ or $\\nu_i<0$ the Lagrangian is unbounded below. Together with $C-\\mu_i-\\nu_i=0$ this also gives $0\\leq \\mu_i\\leq C$.\n",
    "\n",
    "Substituting $a=\\sum_i\\mu_i y_ix_i$ means that the first and second term are the same up to a scalar factor. The 3rd, 4th and 5th term dissappear when minimizing the Lagrangian. The Lagrangian also no longer depends on $\\nu$. This gives dual problem\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{maximize} & \\sum_{i=1}^N\\mu_i - \\frac12 \\sum_i\\sum_j \\mu_iy_i x_i^\\top x_j y_j \\mu_j \\\\\n",
    "\\text{subject to} & 0\\leq \\mu_i\\leq C\\\\\n",
    "& y^\\top \\mu = 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "If we write the data and the labeled together as $Z = (y_1x_1,\\dots,y_Nx_N)\\in\\mathbb R^{n\\times N}$, then the second term in the objective becomes $\\mu^\\top Z^\\top Z \\mu = \\|Z\\mu\\|^2$. Thus we get the QP\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\frac12\\|Z\\mu\\|^2-\\mathbf{1}^\\top\\mu\\\\\n",
    "\\text{subject to} & 0\\leq \\mu_i\\leq C\\\\\n",
    "& y^\\top \\mu = 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "### Exercise 3a)\n",
    "\n",
    "> Write the function `SVM_dual(X,Y,C=1.0)` solving the SVM problem. It should return the optimal values $\\mu$.\n",
    "\n",
    "___  \n",
    "\n",
    "If you get `SolverError` while running the code, pass the `solver='ECOS'` keyword argument to `problem.solve()`; for more information, see the remark below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-backing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1619535480748,
     "user": {
      "displayName": "Rik Voorhaar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiARLX-61EWkT5u3TJ0aLe8HCXUEuzsxfIptxxWrw=s64",
      "userId": "13522764759478134685"
     },
     "user_tz": -120
    },
    "id": "trained-dietary",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2357dc6c21c9d695357731b7aae7da0",
     "grade": true,
     "grade_id": "cell-995e874187f3b48d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "e5ca7813-330c-42ba-b105-8579244dad1c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SVM_dual(X, Y, C=1.0):\n",
    "    n, N = X.shape\n",
    "    Z = X * Y\n",
    "\n",
    "    # ENTER YOUR CODE HERE\n",
    "    \n",
    "\n",
    "\n",
    "X, Y = gen_separable_data(a_true, b_true, N=10, separation_factor=-0.2)\n",
    "C = 100\n",
    "mu = SVM_dual(X, Y, C=C)\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-president",
   "metadata": {
    "id": "interesting-disney"
   },
   "outputs": [],
   "source": [
    "# write tests\n",
    "def tests_3a():\n",
    "    np.random.seed(179)\n",
    "    for C in [10, 100]:\n",
    "        for a_true, b_true in (\n",
    "            (np.array([1, 1]), 0.1),\n",
    "            (np.array([1, 2]), 1),\n",
    "            (np.array([1, 1, 1]), 0),\n",
    "        ):\n",
    "            X, Y = gen_separable_data(a_true, b_true, N=1000, separation_factor=-0.1)\n",
    "            mu = SVM_dual(X, Y, C=C)\n",
    "\n",
    "            def cos_distance(x, y):\n",
    "                return np.abs(np.dot(x, y)) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "            assert (\n",
    "                1 - cos_distance(a_true, (X * Y) @ mu) < 1e-3\n",
    "            ), \"Angle between true hyperplane and computed hyperplane is too big\"\n",
    "\n",
    "            assert np.all(-1e-4 <= mu), \"The constraint is not satisfied\"\n",
    "\n",
    "            assert np.all(mu <= C * 1.001), \"The constraint is not satisfied\"\n",
    "\n",
    "            assert np.abs(mu @ Y) < 1e-4, \"The constraint is not satisfied\"\n",
    "\n",
    "\n",
    "tests_3a()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-messaging",
   "metadata": {
    "id": "varying-commercial"
   },
   "source": [
    "**REMARK**\n",
    "\n",
    "> While the dual is a QP and the primal is an SOCP, surprisingly CVXPY solves the dual more slowly. This is because the solver called by CVXPY converts the objective to the standard form $\\frac12\\mu^\\top (Z^\\top Z)\\mu - 1^\\top \\mu$, but $Z^\\top Z$ is an $N\\times N$ matrix, which is potentially huge. Machine learning libraries implementing SVM's use an iterative algorithm specialized for the particular QP appearing in an SVM. [Read more about this algorithm on Wikipedia](https://en.wikipedia.org/wiki/Sequential_minimal_optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-breast",
   "metadata": {
    "id": "ethical-delicious"
   },
   "source": [
    "After solving the dual problem, we can use the formula $a=\\sum_{i=1}^N\\mu_ix_iy_i=Z\\mu$ to recover the optimal value of $a$ from the optimal values of $\\mu$. Moreover we note that only the points with $\\mu_i>0$ contribute to the hyperplane $a$. Vectors $x_i$ with $\\mu_i>0$ are called _support vectors_, and they are the reason for the name _support vector machine_.\n",
    "\n",
    "The offset $b$ can also be recovered from the dual solution $\\mu$. The support vectors are divided into two classes, those where $0<\\mu_i<C$ are called _on the margin_, and those where $\\mu_i=C$ are _in the margin_. The feasibility conditions of the primal Lagrangian include the condition $\\nu_i\\xi_i=0$ for all $i$, hence $\\xi_i=0$ if $\\nu_i>0$. Since $\\mu_i = C-\\nu_i$, we have that $\\nu_i>0$ precisely if $\\mu_i<C$. Recall that at an optimal point $y_i(a^\\top x_i-b)\\leq 1-\\xi_i$, and for points _on the margin_ ($0<\\mu_i<C$) we precisely have $y_i(a^\\top x_i-b)=1$. These points can thus be used to recover $b$. There is a range of values of $b$ that would work. So in practice we take the _mean_ of $a^\\top x_i-y_i$ for all $x_i$ that lie on the margin to determine $b$.\n",
    "\n",
    "### Exericse 3b)\n",
    "\n",
    "> Write a function `SVM_primal_from_dual(X,Y,mu,C)` that outputs the optimal parameters $(a,b)$ of the primal problem given the solution $\\mu$ to the dual problem. \n",
    "\n",
    "___\n",
    "\n",
    "Tips:  \n",
    "\n",
    "- Due to floating point errors we rarely exactly have $\\mu_i=0$ or $\\mu_i=C$. You can detect $0<\\mu_i<C$ by checking $\\mu_i>10^{-6}$ and $\\mu_i < 0.99  C$, for example. \n",
    "\n",
    "- To get a boolean array with `True` if $a<x<b$ we can use `(a<x) & (x<b)`.\n",
    "\n",
    "- Recall that we can use boolean arrays for indexing. For example `Y[mu==0]` is equivalent to `np.array([Y[i] for i in range(len(Y)) if mu[i]==0])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-floating",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 65620,
     "status": "ok",
     "timestamp": 1619532157551,
     "user": {
      "displayName": "Rik Voorhaar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiARLX-61EWkT5u3TJ0aLe8HCXUEuzsxfIptxxWrw=s64",
      "userId": "13522764759478134685"
     },
     "user_tz": -120
    },
    "id": "secure-chassis",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22760ba8aaec726b3357f25f418d25c8",
     "grade": true,
     "grade_id": "cell-d1400ca2802e1ba7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "a9e8940d-b302-4738-f138-b86fad8c380c"
   },
   "outputs": [],
   "source": [
    "def SVM_primal_from_dual(X, Y, mu, C):\n",
    "    # ENTER YOUR CODE HERE\n",
    "    \n",
    "\n",
    "\n",
    "def mu_sklearn(X, Y, C):\n",
    "    \"\"\"Function to compute mu if 3a isn't solved\"\"\"\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    svc = SVC(C=C, kernel=\"linear\")\n",
    "    svc.fit(X.T, Y)\n",
    "    mu = np.zeros(len(Y)) + np.random.uniform(-1e-13, 1e-13, size=len(Y))\n",
    "    mu[svc.support_] += np.abs(svc.dual_coef_)[0]\n",
    "    return mu\n",
    "\n",
    "\n",
    "X, Y = gen_separable_data(a_true, b_true, separation_factor=-0.2)\n",
    "C = 100\n",
    "try:\n",
    "    mu = SVM_dual(X, Y, C)\n",
    "except:\n",
    "    print(\n",
    "        \"Code for exercise 3a gave an error, we compute `mu` with `sklearn.svm.svc` instead\"\n",
    "    )\n",
    "    mu = mu_sklearn(X, Y, C)\n",
    "\n",
    "a, b = SVM_primal_from_dual(X, Y, mu, C)\n",
    "plot_separating_hyperplane2D(X, Y, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-given",
   "metadata": {
    "id": "decent-desert"
   },
   "outputs": [],
   "source": [
    "# write tests\n",
    "def tests_3b():\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    np.random.seed(179)\n",
    "    for C in [10, 100]:\n",
    "        for a_true, b_true in (\n",
    "            (np.array([1, 1]), 0.1),\n",
    "            (np.array([1, 2]), 1),\n",
    "            (np.array([1, 1, 1]), 0),\n",
    "        ):\n",
    "            X, Y = gen_separable_data(a_true, b_true, N=1000, separation_factor=-0.1)\n",
    "            svc = SVC(C=C, kernel=\"linear\")\n",
    "            svc.fit(X.T, Y)\n",
    "            mu = np.zeros(len(Y)) + np.random.uniform(-1e-13, 1e-13, size=len(Y))\n",
    "            mu[svc.support_] += np.abs(svc.dual_coef_)[0]\n",
    "\n",
    "            a, b = SVM_primal_from_dual(X, Y, mu, C)\n",
    "\n",
    "            def cos_distance(x, y):\n",
    "                return np.abs(np.dot(x, y)) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "            assert (\n",
    "                1 - cos_distance(a, svc.coef_[0]) < 1e-3\n",
    "            ), \"Angle between true hyperplane and computed hyperplane is too big\"\n",
    "\n",
    "            assert svc.intercept_ + b < 1e-3, \"The offset `b` is incorrect\"\n",
    "\n",
    "\n",
    "tests_3b()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-productivity",
   "metadata": {
    "id": "direct-native"
   },
   "source": [
    "## Exercise 4: The kernel trick\n",
    "\n",
    "Recall that the dual is of the form\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\frac12 \\mu^\\top (Z^\\top Z)\\mu-\\mathbf{1}^\\top\\mu\\\\\n",
    "\\text{subject to} & 0\\leq \\mu_i\\leq C\\\\\n",
    "& y^\\top \\mu = 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $(Z^\\top Z)_{ij} = \\langle x_i,\\, x_j\\rangle y_iy_j$. Thus to solve the dual problem we only need to be able to compute inner products between the data vectors and the labels. We can use this to fit more complicated decision boundaries than hyperplanes by applying a nonlinear function $\\phi\\colon \\mathbb R^n\\to \\mathbb R^{m}$ to our data. Then, as decision boundary we can use \n",
    "$$\n",
    "a^\\top \\phi(x) \\leq b\n",
    "$$\n",
    "\n",
    "with $a\\in \\mathbb R^m$ and $b\\in \\mathbb R$. The boundary is a non-linear hypersurface in $\\mathbb R^n$. Another way to interpret this is to regard $\\phi(x)$ as an enrichment or transformation of the original data.\n",
    "\n",
    "For example, for 2D data, we can use a conic section as decision boundary in 2D by mapping $\\phi(s,t)= (1, \\sqrt{2}s,\\sqrt{2}t,s^2,t^2,\\sqrt{2}st)$. Since we only need to compute inner products of points, we never need to compute $\\phi(s,t)$ explicitly since\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\langle \\phi(s,t) ,\\,\\phi(s',t')\\rangle &= \\langle (1, \\sqrt{2}s,\\sqrt{2}t,s^2,t^2,\\sqrt{2}st),\\,(1, \\sqrt{2}s',\\sqrt{2}t',{s'}^2,{t'}^2,\\sqrt{2}s't')\\rangle \\\\ \n",
    "    &= 1+ 2ss'+2tt'+{s}^2{s'}^2+{t}^2{t'}^2+2sts't'\\\\\n",
    "    &= (1+ss'+tt')^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "More generally, to use a basis of degree $d$ polynomials in $\\mathbb R^n$, we can define the symmetric _kernel_ matrix $K$ by\n",
    "$$\n",
    "    K_{ij} = \\left(1+\\langle x_i,\\,x_j\\rangle\\right)^dy_iy_j.\n",
    "$$\n",
    "\n",
    "Another popular choice of kernel are _radial basis functions_ (RBF)  which corresponds to\n",
    "$$\n",
    "K_{ij} = \\exp(-\\gamma\\|x_i-x_j\\|^2)y_iy_j.\n",
    "$$\n",
    "\n",
    "In all cases, the transformed SVM leads to the problem\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\frac12 \\mu^\\top K\\mu-\\mathbf{1}^\\top\\mu\\\\\n",
    "\\text{subject to} & 0\\leq \\mu_i\\leq C\\\\\n",
    "& y^\\top \\mu = 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This is known as the _kernel trick_, since we can model very complicated functions $\\phi$ at the expense of only a little more computational power than 'classical' SVM's. Then from $\\mu$ we can recover $a\\in\\mathbb R^m$ by computing \n",
    "$$\n",
    "a = \\sum_i y_i\\phi(x_i)\\mu_i.\n",
    "$$\n",
    "\n",
    "In practice $\\mu_i$ is zero for the majority of data points, so this is not very expensive to compute. Similarly $b$ can now be obtained by averaging $a^\\top\\phi(x_i)-y_i$ for all points on the margin (i.e. $0<\\mu_i<C$).\n",
    "\n",
    "### Exercise 4a)\n",
    "\n",
    "> Write a function `poly_kernel(X,Y,d)` that computes the kernel matrix corresponding to a polynomial embedding, i.e. $K_{ij} = (1+\\langle x_i,\\,x_j\\rangle)^dy_iy_j$. \n",
    "\n",
    "___\n",
    "\n",
    "Tips:\n",
    "\n",
    "- To create a matrix containing all inner products $\\langle x_i,\\,x_j\\rangle$ you can use $X^\\top X$. You then need to just apply entry-wise operations to this matrix. In particular you need to multiply _entry wise_ by the matrix $yy^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-portrait",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1619552782490,
     "user": {
      "displayName": "Rik Voorhaar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiARLX-61EWkT5u3TJ0aLe8HCXUEuzsxfIptxxWrw=s64",
      "userId": "13522764759478134685"
     },
     "user_tz": -120
    },
    "id": "collected-tourist",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cd2afd1bd5a2a519d8bdd72a658a4b4",
     "grade": true,
     "grade_id": "cell-ec1f9c8407ce5ea6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "8ed6a978-ff21-49fe-b956-2abe259ff2fb"
   },
   "outputs": [],
   "source": [
    "def poly_kernel(X, Y, d):\n",
    "    # ENTER YOUR CODE HERE\n",
    "    \n",
    "\n",
    "\n",
    "X, Y = gen_separable_data(a_true, b_true, N=5)\n",
    "poly_kernel(X, Y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-wireless",
   "metadata": {
    "id": "CSUMULxCrmLf"
   },
   "outputs": [],
   "source": [
    "def tests_4a():\n",
    "    for d in [2, 3]:\n",
    "        for n in [2, 3]:\n",
    "            X = np.ones((n, 100))\n",
    "            Y = np.ones(100)\n",
    "            K = poly_kernel(X, Y, d)\n",
    "            assert (\n",
    "                np.sum(np.abs(K - (n + 1) ** d)) < 1e-10,\n",
    "                f\"Incorrect result for n={n}, d={d}\",\n",
    "            )\n",
    "\n",
    "            X, Y = gen_separable_data([1] * n, 0)\n",
    "            K = poly_kernel(X, Y, d)\n",
    "            assert np.linalg.norm(K.T - K) < 1e-5, \"The kernel is not symmetric\"\n",
    "            for (i, j) in np.random.choice(100, size=(10, 2)):\n",
    "                assert (\n",
    "                    np.abs(K[i, j] - (1 + np.dot(X[:, i], X[:, j])) ** d) * Y[i] * Y[j]\n",
    "                    < 1e-6\n",
    "                ), f\"Incorrect result for n={n}, d={d}\"\n",
    "\n",
    "\n",
    "tests_4a()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-voice",
   "metadata": {
    "id": "3Z_imi30RRri"
   },
   "source": [
    "**REMARK**\n",
    "\n",
    "> To compute $a$ from the optimal values $\\mu$, we will still need a map $\\phi$ giving the polynomial features. For degree $2$ this was easy to deduce, but for degree $d$ it is a bit tricky. The map $\\phi$ should satisfy\n",
    "$$\n",
    "\\left(1+\\sum_{k=1}^n x_i[k]x_j[k]\\right)^d = \\left\\langle\\phi(x_i),\\,\\phi(x_j)\\right\\rangle\n",
    "$$  \n",
    "\n",
    "> Where $x_i[k]$ is the $k$th entry of $x_i$. If for convenience we define $x_i[0]=1$, then we can expand the left side using the multinomial theorem to \n",
    "$$\n",
    "\\sum_{m_0+\\dots+m_n=d}\\binom{d}{m_0,\\dots,m_n}\\prod_{k=0}^n x_i[k]^{m_k}x_j[k]^{m_k}\n",
    "$$  \n",
    "\n",
    "> with $m_\\ell\\geq 0$. From this expression we can deduce that $\\phi$ is given by\n",
    "$$\n",
    "\\phi(x_i) = \\left(\\left.\\sqrt{\\binom{d}{m_0,\\dots,m_n}}\\prod_{k=1}^n x_i[k]^{m_k} \\,\\right|\\,m_0+\\dots+m_n=d\\right)\n",
    "$$  \n",
    "\n",
    "> We can generate a list of all multinomial coefficients using the `sympy` library, specifcally the function [sympy.ntheory.multinomial_coefficients](https://docs.sympy.org/latest/modules/ntheory.html#sympy.ntheory.multinomial.multinomial_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-eclipse",
   "metadata": {
    "id": "q3ex71TpBK6m"
   },
   "outputs": [],
   "source": [
    "def polynomial_features(X, d):\n",
    "    \"\"\"Computes the polynomial features of degree `d` with correct multinomial coefficients.\n",
    "    Here `X` is an array of shape `(n,N)`, with `N` the number of data points and `n` the dimensionality.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # Generate list of all powers of X\n",
    "    X_powers = np.stack([X ** i for i in range(d + 1)])\n",
    "\n",
    "    # Get all multinomial coefficients\n",
    "    # Store tuples (m_1,...,m_n) in array `powers` (m_0 is ignored)\n",
    "    # Store coefficients in `coeffs` and take square root.\n",
    "    multinomial = sympy.ntheory.multinomial_coefficients(n + 1, d)\n",
    "    powers = np.array(list(multinomial.keys()))\n",
    "    powers = powers[:, 1:]\n",
    "    coeffs = np.sqrt(np.array(list(multinomial.values()), float))\n",
    "\n",
    "    # Compute the product x_i[k]^{m_k} for each data point, and each tuple (m_1,..,m_n)\n",
    "    # Multiply result by the square roots of multinomial coefficients\n",
    "    features = np.prod([X_powers[powers[:, i], i] for i in range(n)], axis=0).T\n",
    "    features = coeffs * features\n",
    "    return features.T\n",
    "\n",
    "\n",
    "def plot_poly_hypersurface(X, Y, a, b, d, N_points_contour=20):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(X[0][Y == -1], X[1][Y == -1], \".\")\n",
    "    plt.plot(X[0][Y == 1], X[1][Y == 1], \".\")\n",
    "    plt.ylim(np.min(X[1]) - 0.1, np.max(X[1]) + 0.1)\n",
    "    plt.xlim(np.min(X[0]) - 0.1, np.max(X[0]) + 0.1)\n",
    "\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(np.min(X[0]) - 0.1, np.max(X[0]) + 0.1, N_points_contour),\n",
    "        np.linspace(np.min(X[1]) - 0.1, np.max(X[1]) + 0.1, N_points_contour),\n",
    "    )\n",
    "    xxyy = np.stack([xx, yy]).reshape(2, -1)\n",
    "    values = a @ polynomial_features(xxyy, d) - b\n",
    "    values = values.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, values, [0], c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-landscape",
   "metadata": {
    "id": "egyptian-execution"
   },
   "source": [
    "### Exercise 4b)\n",
    "\n",
    "> Write a function `SVM_poly(X,Y,d,C=1.0)` that solves the SVM problem using the kernel trick with a polynomial kernel of degree $d$. The function should return the values for $(a,b)$ in a tuple.\n",
    "\n",
    "___\n",
    "\n",
    "Tips:\n",
    "\n",
    "- Use [cp.quad_form](https://www.cvxpy.org/api_reference/cvxpy.atoms.other_atoms.html#quad-form) for the expression $\\mu^\\top K\\mu$.\n",
    "\n",
    "- Use `polynomial_features(X,d)` to compute $\\phi(x_i)$ needed for the formula $a=\\sum_{i=1}^Ny_i\\phi(x_i)\\mu_i$. This is also needed for computing the offset $b=\\mathbb E[a^\\top x -y\\mid0<\\mu<C] $\n",
    "\n",
    "- If you did not solve the previous exercise, you can instead use `K=Z.T@Z` with `Z = polynomial_features(X,d)*Y` instead. (For low-dimensional data this is not much slower than the kernel trick, for high-dimensional data it matters a lot since the dimension of the polynomial embedding scales as $n^d$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-dayton",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 1161,
     "status": "ok",
     "timestamp": 1619555086760,
     "user": {
      "displayName": "Rik Voorhaar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiARLX-61EWkT5u3TJ0aLe8HCXUEuzsxfIptxxWrw=s64",
      "userId": "13522764759478134685"
     },
     "user_tz": -120
    },
    "id": "G0gaOzn0xFCJ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "221bdd3677c239d34dfea55b49fce751",
     "grade": true,
     "grade_id": "cell-4656f0966983e6fb",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "009ec5ad-d71d-42c8-9f7d-d5d7dc18f82e"
   },
   "outputs": [],
   "source": [
    "def SVM_poly(X, Y, d, C=1.0):\n",
    "    n, N = X.shape\n",
    "\n",
    "    # ENTER YOUR CODE HERE\n",
    "    \n",
    "\n",
    "\n",
    "d = 2\n",
    "N = 500\n",
    "X = np.random.uniform(-1, 1, (2, N))\n",
    "(s1, s2) = np.random.uniform(-1, 1, 2)\n",
    "F = s1 * X[0] ** 2 + s2 * X[1] ** 2\n",
    "Y = F < np.median(F)\n",
    "Y = Y.astype(int) * 2 - 1\n",
    "a, b = SVM_poly(X, Y, d, 100)\n",
    "\n",
    "plot_poly_hypersurface(X, Y, a, b, d, N_points_contour=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-asbestos",
   "metadata": {
    "id": "qoQ1HTnvY712"
   },
   "outputs": [],
   "source": [
    "def tests_4b():\n",
    "    np.random.seed(179)\n",
    "\n",
    "    def classification_error(X, Y, a, b):\n",
    "        return np.mean((a @ polynomial_features(X, d) - b > 0) != (Y == 1))\n",
    "\n",
    "    for d in [1, 2, 3]:\n",
    "        for n in [2, 3, 4]:\n",
    "            N = 100\n",
    "            X = np.random.uniform(-1, 1, (n, N))\n",
    "            F = np.sum((np.random.uniform(-1, 1, n) * X.T) ** d, axis=1)\n",
    "            Y = F < np.median(F)\n",
    "            Y = Y.astype(int) * 2 - 1\n",
    "\n",
    "            a, b = SVM_poly(X, Y, d, 100)\n",
    "            error = classification_error(X, Y, a, b)\n",
    "            if n == 2:\n",
    "                assert error < 0.1, \"Classfication error is too big\"\n",
    "            else:\n",
    "                assert error < 0.5, \"Classfication error is too big\"\n",
    "\n",
    "\n",
    "tests_4b()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('optimizationII': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}